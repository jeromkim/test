# ì±„íŒ… ë°ì´í„°í”Œë¡œìš° ìƒì„¸ í”„ë¡œì„¸ìŠ¤

## ğŸ”„ ì „ì²´ ì±„íŒ… í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant ST as Streamlit Frontend
    participant FA as FastAPI Backend
    participant GR as Bedrock Guardrail
    participant LLM as Bedrock LLM
    participant KB as Knowledge Base
    participant DB as DynamoDB
    participant SM as Secrets Manager

    %% 1. ì¸ì¦ ë‹¨ê³„
    User->>ST: JWT í† í°ìœ¼ë¡œ ì ‘ì†
    ST->>FA: POST /api/auth/verify
    FA->>SM: JWT ì‹œí¬ë¦¿ ì¡°íšŒ
    SM-->>FA: ì‹œí¬ë¦¿ ë°˜í™˜
    FA-->>ST: ì‚¬ìš©ì ì •ë³´ ë°˜í™˜
    ST-->>User: ì¸ì¦ ì„±ê³µ, ì±„íŒ… í™”ë©´

    %% 2. ì±„íŒ… ìš”ì²­ ë‹¨ê³„
    User->>ST: ë©”ì‹œì§€ ì…ë ¥
    ST->>FA: POST /api/chat (ìŠ¤íŠ¸ë¦¬ë°)
    
    %% 3. ë³´ì•ˆ ê²€ì¦
    FA->>GR: ì…ë ¥ ë©”ì‹œì§€ Guardrail ì²´í¬
    GR-->>FA: ì•ˆì „ì„± ê²€ì¦ ê²°ê³¼
    
    alt ì°¨ë‹¨ëœ ê²½ìš°
        FA-->>ST: ì°¨ë‹¨ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
        ST-->>User: ì°¨ë‹¨ ì•Œë¦¼ í‘œì‹œ
    else í†µê³¼í•œ ê²½ìš°
        %% 4. íˆìŠ¤í† ë¦¬ ë¡œë“œ
        FA->>DB: ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        DB-->>FA: ì´ì „ ëŒ€í™” ë‚´ì—­
        
        %% 5. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        FA->>DB: ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        
        %% 6. LLM ì²˜ë¦¬ ì‹œì‘
        FA->>LLM: ë©”ì‹œì§€ + íˆìŠ¤í† ë¦¬ ì „ì†¡
        
        %% 7. ë„êµ¬ í˜¸ì¶œ (Knowledge Search)
        LLM->>FA: ë„êµ¬ í˜¸ì¶œ ìš”ì²­
        FA->>KB: ì§€ì‹ ê²€ìƒ‰ ì‹¤í–‰
        KB-->>FA: ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
        FA->>FA: ê²°ê³¼ ë¦¬ë­í‚¹ ë° ìµœì í™”
        FA->>LLM: ë„êµ¬ ê²°ê³¼ ì „ì†¡
        
        %% 8. ìµœì¢… ì‘ë‹µ ìƒì„±
        LLM-->>FA: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        FA->>DB: AI ì‘ë‹µ ì €ì¥
        FA-->>ST: SSE ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡
        ST-->>User: ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ
    end
```

## ğŸ“Š ë‹¨ê³„ë³„ ìƒì„¸ í”„ë¡œì„¸ìŠ¤

### 1ï¸âƒ£ **ì¸ì¦ ë‹¨ê³„** (app.py â†’ auth.py)

```python
# 1. í† í° ì¶”ì¶œ
query_params = st.query_params
token = query_params.get("token", None)

# 2. í† í° ê²€ì¦
def verify_token(token: str) -> bool:
    response = requests.post(f"{FASTAPI_URL}/api/auth/verify", json={"token": token})
    # JWT ë””ì½”ë”© ë° ì‚¬ìš©ì ì •ë³´ ì¶”ì¶œ

# 3. ì„¸ì…˜ ìƒíƒœ ì„¤ì •
st.session_state["user_id"] = user_info["user_id"]
st.session_state["last_active"] = datetime.now(timezone.utc)
```

### 2ï¸âƒ£ **ì±„íŒ… ìš”ì²­ ë‹¨ê³„** (chat.py â†’ request.py â†’ main.py)

```python
# Frontend: ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input():
    user_message_id = st.session_state.message_counter
    st.session_state.message_counter += 1
    st.session_state.message_list.append({
        "role": "user", 
        "content": user_question, 
        "id": user_message_id
    })

# API í˜¸ì¶œ
response_chunks = call_fastapi_predict(
    user_question,
    session_id,
    model_id,
    user_message_id,
    ai_response_message_id,
    user_prompt
)
```

### 3ï¸âƒ£ **ë³´ì•ˆ ê²€ì¦ ë‹¨ê³„** (main.py â†’ Guardrail)

```python
# Guardrail ì…ë ¥ ê²€ì¦
async def check_input_guardrail(user_message_content: str):
    bedrock_client = bedrock_clients['ap-northeast-2']
    response = await bedrock_client.apply_guardrail(
        guardrailIdentifier=config.guardrail_id,
        guardrailVersion=config.guardrail_version,
        source='INPUT',
        content=[{'text': {'text': user_message_content}}]
    )
    return response

# ì°¨ë‹¨ ì‹œ ì²˜ë¦¬
if guardrail_response["action"] == "GUARDRAIL_INTERVENED":
    return await create_guardrail_blocked_response(guardrail_response)
```

### 4ï¸âƒ£ **íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë‹¨ê³„** (chat_controller.py â†’ history_service.py)

```python
# íˆìŠ¤í† ë¦¬ ë¡œë“œ
history_service = HistoryService(session_id=session_id, user_ip=user_ip)
loaded_messages: List[BaseMessage] = await history_service.get_messages()

# ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
await history_service.add_user_message(user_message_content, user_message_id)

# tool_calls ì œê±° (íˆìŠ¤í† ë¦¬ ì¬ì‚¬ìš© ì‹œ ì—ëŸ¬ ë°©ì§€)
cleaned_messages = []
for msg in messages:
    if isinstance(msg, AIMessage):
        # contentì—ì„œ toolUse ë¸”ë¡ ì œê±°
        cleaned_content = [
            block for block in msg.content 
            if not (isinstance(block, dict) and block.get('type') in ['tool_use', 'toolUse'])
        ]
```

### 5ï¸âƒ£ **LLM ì²˜ë¦¬ ë‹¨ê³„** (chat_service.py)

```python
# ë©”ì‹œì§€ êµ¬ì„±
def build_messages(self, loaded_history, user_message_content, group):
    return [
        self._build_system_prompt(),  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        *loaded_history,              # ì´ì „ ëŒ€í™”
        HumanMessage(content=user_message_content.strip())  # í˜„ì¬ ì…ë ¥
    ]

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
async def generate_streaming_response(self, messages, ai_response_message_id, group):
    while True:  # ë„êµ¬ í˜¸ì¶œ ë£¨í”„
        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        async for chunk in self.llm.astream(messages + current_messages):
            if chunk.content:
                yield f"data: {json.dumps({'role': 'assistant', 'content': content})}\n\n"
        
        # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
        if ai_message.tool_calls:
            for tool_call in ai_message.tool_calls:
                tool_result = self.tool_dict[tool_name](**tool_args)
                yield f"data: {json.dumps({'role': 'tool', 'content': tool_result})}\n\n"
        else:
            break  # ë„êµ¬ í˜¸ì¶œ ì—†ìœ¼ë©´ ì¢…ë£Œ
```

### 6ï¸âƒ£ **ì§€ì‹ ê²€ìƒ‰ ë‹¨ê³„** (retrieve_knowledge_search.py)

```python
# 1. Knowledge Base ê²€ìƒ‰
def retrieve_knowledge_base_search(query: str, group: str):
    response = bedrock_agent_runtime_client.retrieve(
        knowledgeBaseId=knowledge_id,
        retrievalConfiguration={
            "vectorSearchConfiguration": {
                "numberOfResults": 5,
                "overrideSearchType": "HYBRID"
            }
        },
        retrievalQuery={"text": query}
    )
    
    # 2. ê²°ê³¼ ë¦¬ë­í‚¹ (Cohere)
    reranked = rerank_results(query, results, top_k=5)
    
    # 3. ë™ì  ê¸¸ì´ ìµœì í™”
    optimized = apply_dynamic_truncation(reranked)
    
    return optimized
```

### 7ï¸âƒ£ **ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ë‹¨ê³„** (chat_controller.py â†’ Frontend)

```python
# Backend: SSE ìŠ¤íŠ¸ë¦¬ë°
async def stream_generator():
    async for item in chat_service.generate_streaming_response():
        if isinstance(item, BaseMessage):
            await history_service.add_message(item, item_message_id)
        elif isinstance(item, str):
            yield item  # SSE ë°ì´í„° ì „ì†¡

# Frontend: ì‹¤ì‹œê°„ ìˆ˜ì‹ 
for chunk in response_chunks:
    if isinstance(chunk, dict) and chunk.get("role") == "assistant":
        full_response += chunk["content"]
        placeholder.write(full_response)  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
```

## ğŸ” ë°ì´í„° êµ¬ì¡° ìƒì„¸

### **ë©”ì‹œì§€ êµ¬ì¡°**
```python
# ì‚¬ìš©ì ë©”ì‹œì§€
{
    "role": "user",
    "content": "ì§ˆë¬¸ ë‚´ìš©",
    "id": 123,
    "timestamp": "2024-11-24T10:00:00Z"
}

# AI ì‘ë‹µ
{
    "role": "assistant", 
    "content": "ë‹µë³€ ë‚´ìš©",
    "id": 124,
    "tool_calls": [...],  # ë„êµ¬ í˜¸ì¶œ ì •ë³´
    "source_data": [...]  # ê²€ìƒ‰ ê²°ê³¼
}
```

### **ë„êµ¬ í˜¸ì¶œ êµ¬ì¡°**
```python
{
    "name": "retrieve_knowledge_base_search",
    "args": {"query": "ì‚¬ìš©ì ì§ˆë¬¸", "group": "common"},
    "id": "call_abc123"
}
```

### **ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°**
```python
{
    "content": {"text": "ë¬¸ì„œ ë‚´ìš©"},
    "metadata": {"x-amz-bedrock-kb-document-page-number": 1},
    "location": {"s3Location": {"uri": "s3://bucket/file.pdf"}},
    "score": 0.85
}
```

## âš¡ ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸

1. **í† í° ì ˆì•½**: ë™ì  ë¬¸ì„œ ê¸¸ì´ ì¡°ì •
2. **ì‘ë‹µ ì†ë„**: ìŠ¤íŠ¸ë¦¬ë° + ë¹„ë™ê¸° ì²˜ë¦¬
3. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: tool_calls ì œê±°, íˆìŠ¤í† ë¦¬ ì •ë¦¬
4. **ê²€ìƒ‰ ì •í™•ë„**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹